{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c992aab-e2d8-4381-8fe6-7c3932f92d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe94adfd",
   "metadata": {},
   "source": [
    "## Starter Code from Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bd309e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__label__1 Love the magnet easel', '__label__1  great for moving to different areas', '__label__1  Wish it had some sort of non skid pad on bottom though', '__label__1 Both sides are magnetic', \"__label__1  A real plus when you're entertaining more than one child\", '__label__1  The four-year old can find the letters for the words, while the two-year old can find the pictures the words spell', '__label__1  (I bought letters and magnetic pictures to go with this board)', '__label__1  Both grandkids liked it a lot, which means I like it a lot as well', '__label__1  Have not even introduced markers, as this will be used strictly as a magnetic board']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function below unzips the archive to the local directory. \n",
    "\n",
    "def unzip_data(input_data_path):\n",
    "    with zipfile.ZipFile(input_data_path, 'r') as input_data_zip:\n",
    "        input_data_zip.extractall('.')\n",
    "\n",
    "# Input data is a file with a single JSON object per line with the following format: \n",
    "# {\n",
    "#  \"reviewerID\": <string>,\n",
    "#  \"asin\": <string>,\n",
    "#  \"reviewerName\" <string>,\n",
    "#  \"helpful\": [\n",
    "#    <int>, (indicating number of \"helpful votes\")\n",
    "#    <int>  (indicating total number of votes)\n",
    "#  ],\n",
    "#  \"reviewText\": \"<string>\",\n",
    "#  \"overall\": <int>,\n",
    "#  \"summary\": \"<string>\",\n",
    "#  \"unixReviewTime\": <int>,\n",
    "#  \"reviewTime\": \"<string>\"\n",
    "# }\n",
    "# \n",
    "# We are specifically interested in the fields \"helpful\" and \"reviewText\"\n",
    "#\n",
    "\n",
    "def label_data(input_data):\n",
    "    labeled_data = []\n",
    "    HELPFUL_LABEL = \"__label__1\"\n",
    "    UNHELPFUL_LABEL = \"__label__2\"\n",
    "     \n",
    "    for l in open(input_data, 'r'):\n",
    "        l_object = json.loads(l)\n",
    "        helpful_votes = float(l_object['helpful'][0])\n",
    "        total_votes = l_object['helpful'][1]\n",
    "        reviewText = l_object['reviewText']\n",
    "        if total_votes != 0:\n",
    "            if helpful_votes / total_votes > .5:\n",
    "                labeled_data.append(\" \".join([HELPFUL_LABEL, reviewText]))\n",
    "            elif helpful_votes / total_votes < .5:\n",
    "                labeled_data.append(\" \".join([UNHELPFUL_LABEL, reviewText]))\n",
    "          \n",
    "    return labeled_data\n",
    "\n",
    "\n",
    "# Labeled data is a list of sentences, starting with the label defined in label_data. \n",
    "\n",
    "def split_sentences(labeled_data):\n",
    "    split_sentences = []\n",
    "    for d in labeled_data:\n",
    "        label = d.split()[0]        \n",
    "        sentences = \" \".join(d.split()[1:]).split(\".\") # Initially split to separate label, then separate sentences\n",
    "        for s in sentences:\n",
    "            if s: # Make sure sentences isn't empty. Common w/ \"...\"\n",
    "                split_sentences.append(\" \".join([label, s]))\n",
    "    return split_sentences\n",
    "\n",
    "\n",
    "input_data  = unzip_data('Toys_and_Games_5.json.zip')\n",
    "labeled_data = label_data('Toys_and_Games_5.json')\n",
    "split_sentence_data = split_sentences(labeled_data)\n",
    "\n",
    "print(split_sentence_data[0:9])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b9518",
   "metadata": {},
   "source": [
    "## Exercise Solution: Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980d3684",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "write_trainfile() missing 1 required positional argument: 'split_sentence_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m split_data_validationlen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(split_sentence_data) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m.1\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Todo: write the training file\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m train_path \u001b[38;5;241m=\u001b[39m \u001b[43mwrite_trainfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining file written!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Todo: write the validation file\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: write_trainfile() missing 1 required positional argument: 'split_sentence_data'"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "# Note: This solution implies that the bucket below has already been made and that you have access\n",
    "# to that bucket. You would need to change the bucket below to a bucket that you have write\n",
    "# premissions to. This will take time depending on your internet connection, the training file is ~ 40 mb\n",
    "\n",
    "BUCKET = \"udacity-sagemaker-solutiondata2021\"\n",
    "s3_prefix = \"l2e1\"\n",
    "\n",
    "def cycle_data(fp, data):\n",
    "    for d in data:\n",
    "        fp.write(d + \"\\n\")\n",
    "\n",
    "def write_trainfile(split_sentence_data):\n",
    "    train_path = \"hello_blaze_train\"\n",
    "    with open(train_path, 'w') as f:\n",
    "        cycle_data(f, split_sentence_data)\n",
    "    return train_path\n",
    "\n",
    "def write_validationfile(split_sentence_data):\n",
    "    validation_path = \"hello_blaze_validation\"\n",
    "    with open(validation_path, 'w') as f:\n",
    "        cycle_data(f, split_sentence_data)\n",
    "    return validation_path \n",
    "\n",
    "def upload_file_to_s3(file_name, s3_prefix):\n",
    "    object_name = os.path.join(s3_prefix, file_name)\n",
    "    s3_client = boto3.client('s3')\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_name, BUCKET, object_name)\n",
    "    except ClientError as e:\n",
    "        logging.error(e)\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "split_data_trainlen = int(len(split_sentence_data) * .9)\n",
    "split_data_validationlen = int(len(split_sentence_data) * .1)\n",
    "\n",
    "\n",
    "train_path = write_trainfile(split_sentence_data[:split_data_trainlen])\n",
    "print(\"Training file written!\")\n",
    "validation_path = write_validationfile(split_sentence_data[split_data_trainlen:])\n",
    "print(\"Validation file written!\")\n",
    "\n",
    "upload_file_to_s3(train_path, s3_prefix)\n",
    "print(\"Train file uploaded!\")\n",
    "upload_file_to_s3(validation_path, s3_prefix)\n",
    "print(\"Validation file uploaded!\")\n",
    "\n",
    "print(\" \".join([train_path, validation_path]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de070b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
